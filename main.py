# -*- coding: utf-8 -*-
#
# åŠŸèƒ½æ¨¡å—å’Œå®ç°æ‰‹æ³•æ•´ç†
#
# æœ¬è„šæœ¬æ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–æµè§ˆå™¨ï¼ˆPlaywrightï¼‰æŠ“å– Civitai.com å›¾ç‰‡æœç´¢ç»“æœé¡µé¢çš„ä¿¡æ¯ï¼Œ
# åŒ…æ‹¬æ¯ä¸ªæœç´¢ç»“æœçš„å›¾ç‰‡ç¼©ç•¥å›¾ï¼ˆå¹¶ä¸‹è½½åˆ°æœ¬åœ°ï¼‰ã€å…¶å…³è”çš„åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥ï¼Œ
# ä»¥åŠ5ä¸ªè¡¨æƒ…ç¬¦å·ï¼ˆç‚¹èµã€çˆ±å¿ƒã€ç¬‘å“­ã€ä¼¤å¿ƒã€æ‰“èµï¼‰åŠå…¶å¯¹åº”çš„æ•°å€¼ã€‚
# æŠ“å–åˆ°çš„æ•°æ®å°†ä¸æŠ“å–æ—¶é—´æˆ³ã€æœç´¢å…³é”®è¯ï¼ˆæœ¬æ¬¡å›ºå®šä¸º`tags=4`ï¼‰å’Œæœç´¢URLä¸€åŒè¢«æ•´ç†å¹¶å¯¼å‡ºåˆ°Excelæ–‡ä»¶ä¸­ã€‚
# æ¯ä¸ªå›¾ç‰‡å°†ä¿å­˜åˆ°å…¶ä¸“å±çš„æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ï¼Œå¹¶åœ¨Excelä¸­æä¾›æœ¬åœ°å›¾ç‰‡æ–‡ä»¶çš„ç»å¯¹è·¯å¾„å’Œå¯ç‚¹å‡»çš„è¶…é“¾æ¥ã€‚
# è„šæœ¬ä¼šä¿ç•™åŸæœ‰çš„æ—¥å¿—ç³»ç»Ÿï¼Œè‡ªåŠ¨æ‰“å¼€æ—¥å¿—å’Œç»“æœæ–‡ä»¶ï¼Œå¹¶æ”¯æŒæ³¨å…¥Cookiesã€‚
#
# --- ä¸»è¦åŠŸèƒ½æ¨¡å— ---
#
# 1. é…ç½®ç®¡ç† (Configuration):
#    - PROXY: å®šä¹‰HTTPä»£ç†åœ°å€ï¼Œç”¨äºPlaywrightæµè§ˆå™¨å’Œaiohttpåº“çš„ç½‘ç»œè¯·æ±‚ï¼Œè§„é¿åœ°ç†é™åˆ¶æˆ–æé«˜è®¿é—®ç¨³å®šæ€§ã€‚
#    - LOG_DIR, IMAGE_DIR_BASE, RESULTS_DIR: å®šä¹‰æ—¥å¿—æ–‡ä»¶ã€å›¾ç‰‡å­˜å‚¨å’ŒExcelç»“æœæ–‡ä»¶çš„è¾“å‡ºç›®å½•ã€‚
#    - KEYWORD_TARGET_FILE: å­˜å‚¨å¾…æœç´¢çš„å…³é”®è¯åˆ—è¡¨çš„æ–‡ä»¶ (åœ¨æ­¤è„šæœ¬ä¸­ï¼ŒCivitai URLå›ºå®šï¼Œæ­¤æ–‡ä»¶ç”¨äºæ¼”ç¤ºï¼Œå®é™…ä¸ç”¨äºåŠ¨æ€æœç´¢).
#    - ç›®å½•åˆ›å»º: è„šæœ¬å¯åŠ¨æ—¶è‡ªåŠ¨åˆ›å»ºæ‰€éœ€çš„æ—¥å¿—ã€å›¾ç‰‡å’Œç»“æœç›®å½•ï¼Œç¡®ä¿æ–‡ä»¶èƒ½æ­£ç¡®ä¿å­˜ã€‚
#    - æ–‡ä»¶å‘½å: ä½¿ç”¨æ—¶é—´æˆ³ä¸ºæ—¥å¿—å’ŒExcelæ–‡ä»¶ç”Ÿæˆå”¯ä¸€åç§°ã€‚
#
# 2. æ—¥å¿—ç³»ç»Ÿ (Logging Setup):
#    - ä½¿ç”¨Pythonå†…ç½®çš„ `logging` æ¨¡å—ï¼Œé…ç½®æ—¥å¿—è®°å½•å™¨ `civitai_scraper`ã€‚
#    - æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º `INFO`ï¼Œè®°å½•é‡è¦æ“ä½œå’Œä¿¡æ¯ã€‚
#    - åŒæ—¶é…ç½®æ–‡ä»¶å¤„ç†å™¨ (`FileHandler`) å’Œæ§åˆ¶å°å¤„ç†å™¨ (`StreamHandler`)ï¼Œå®ç°æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°ã€‚
#    - æ—¥å¿—æ ¼å¼åŒ–: å®šä¹‰ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºæ ¼å¼ã€‚
#    - ç›®çš„: æ–¹ä¾¿è·Ÿè¸ªè„šæœ¬è¿è¡ŒçŠ¶æ€ã€è°ƒè¯•é—®é¢˜å’Œè®°å½•æŠ“å–è¿‡ç¨‹ä¸­çš„äº‹ä»¶ã€‚
#    - è‡ªåŠ¨æ‰“å¼€æ—¥å¿—: è„šæœ¬è¿è¡Œç»“æŸåï¼Œä¼šè‡ªåŠ¨å°è¯•æ‰“å¼€æœ¬æ¬¡è¿è¡Œç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶ã€‚
#
# 3. å…³é”®è¯è¯»å– (Keyword Reading Helper Function - `read_keywords_from_file`):
#    - ç›®çš„: ä» `keywordTarget.txt` æ–‡ä»¶ä¸­è¯»å–å¾…æœç´¢çš„å…³é”®è¯åˆ—è¡¨ã€‚
#    - å®ç°æ‰‹æ³•: æŒ‰è¡Œè¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ¯è¡Œè§†ä¸ºä¸€ä¸ªå…³é”®è¯ï¼Œå¹¶è¿›è¡ŒåŸºæœ¬æ ¡éªŒã€‚
#    - ä¼˜ç‚¹: æé«˜è„šæœ¬çš„çµæ´»æ€§å’Œå¯é…ç½®æ€§ã€‚ (åœ¨æ­¤è„šæœ¬ä¸­ï¼Œç”±äºCivitai URLå›ºå®šï¼Œæ­¤å‡½æ•°ä»…ä¸ºä¿ç•™ç»“æ„)
#
# 4. æµè§ˆå™¨è‡ªåŠ¨åŒ–ä¸æ•°æ®æŠ“å– (Browser Automation & Data Scraping - `performCivitaiImageScrape`):
#    - æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£å®é™…çš„ç½‘é¡µäº¤äº’å’Œæ•°æ®æå–ã€‚
#    - å¼‚æ­¥æ“ä½œ: ä½¿ç”¨ `asyncio` å’Œ `playwright.async_api` å®ç°å¼‚æ­¥å¹¶å‘æŠ“å–ã€‚
#    - æµè§ˆå™¨ä¸Šä¸‹æ–‡ç®¡ç†: ä¸ºæ¯ä¸ªæŠ“å–ä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ã€‚
#    - æ³¨å…¥ Cookies: å°è¯•ä» `cookies.json` æ–‡ä»¶ä¸­è¯»å–å¹¶æ³¨å…¥ cookiesï¼Œä»¥ç»´æŒä¼šè¯çŠ¶æ€ã€‚
#    - é¡µé¢å¯¼èˆª: å¯¼èˆªåˆ°Civitaiå›¾ç‰‡æœç´¢URLï¼ˆ`https://civitai.com/images?tags=4`ï¼‰ã€‚
#    - åŠ¨æ€å†…å®¹åŠ è½½ (æ»šåŠ¨): å®ç°ç€‘å¸ƒæµé¡µé¢çš„åŠ¨æ€åŠ è½½ï¼Œé€šè¿‡æ»šåŠ¨é¡µé¢è§¦å‘æ–°å†…å®¹ã€‚
#    - æ•°æ®æå– (ä½¿ç”¨BeautifulSoupè¾…åŠ©):
#      - å®šä½å¤§çš„å›¾ç‰‡ç»“æœå…ƒç´ ï¼ˆåŸºäºæä¾›çš„CSSé€‰æ‹©å™¨ï¼‰ã€‚
#      - æå–ç¼©ç•¥å›¾ (`img` æ ‡ç­¾çš„ `src`)ã€‚
#      - æå–åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥ (`img` æ ‡ç­¾çš„çˆ¶çº§ `a` æ ‡ç­¾çš„ `href`)ã€‚
#      - æå–5ä¸ªè¡¨æƒ…åŠå…¶æ•°å€¼ (ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ)ã€‚
#    - å›¾ç‰‡ä¸‹è½½: ä½¿ç”¨ `aiohttp` å¼‚æ­¥ä¸‹è½½ç¼©ç•¥å›¾ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚
#      - æ–‡ä»¶å‘½å: ä½¿ç”¨æ—¶é—´æˆ³å’Œå›¾ç‰‡URLçš„SHA256å“ˆå¸Œå€¼ï¼Œç¡®ä¿å”¯ä¸€æ€§ã€‚
#    - æ•°æ®å­˜å‚¨: å°†æŠ“å–åˆ°çš„æ•°æ®å­˜å‚¨åˆ°å…¨å±€åˆ—è¡¨ `all_search_results_data` ä¸­ã€‚
#    - çº¿ç¨‹å®‰å…¨: ä½¿ç”¨ `asyncio.Lock` ä¿æŠ¤å…¨å±€å…±äº«æ•°æ®ç»“æ„ã€‚
#    - **å»é‡æœºåˆ¶**: ä½¿ç”¨ `download_history` å­—å…¸å’Œå›¾ç‰‡å†…å®¹çš„ MD5 å“ˆå¸Œå€¼ï¼Œé¿å…é‡å¤ä¸‹è½½å’Œå­˜å‚¨ç›¸åŒçš„å›¾ç‰‡æ•°æ®ã€‚
#    - **å…³é”®è¯æå–**: å°è¯•ä»é¡µé¢è¾“å…¥æ¡†ä¸­æå–å…³é”®è¯ã€‚
#
# 5. ä¸»æ‰§è¡Œé€»è¾‘ (Main Execution Logic - `main`):
#    - å¯åŠ¨Playwrightæµè§ˆå™¨ï¼Œå¹¶è®¾ç½®å›ºå®šçš„çª—å£åˆ†è¾¨ç‡ã€‚
#    - (å¯é€‰) è°ƒç”¨ `read_keywords_from_file` è·å–æ‰€æœ‰ç›®æ ‡å…³é”®è¯ (åœ¨æ­¤è„šæœ¬ä¸­ï¼Œä»…ä¸ºç»“æ„ä¿ç•™)ã€‚
#    - ä¸ºæ¯ä¸ªå…³é”®è¯ï¼ˆæˆ–æœ¬æ¬¡å›ºå®šçš„Civitai URLï¼‰åˆ›å»º `performCivitaiImageScrape` ä»»åŠ¡ï¼Œå¹¶å¹¶å‘æ‰§è¡Œã€‚
#    - å…³é—­æµè§ˆå™¨å®ä¾‹ã€‚
#
# 6. Excelæ•°æ®å¯¼å‡º (Excel Export Logic):
#    - ä½¿ç”¨ `openpyxl` åº“åˆ›å»ºæ–°çš„Excelå·¥ä½œç°¿ï¼Œå¹¶åˆ›å»ºåä¸º "Civitaiå›¾ç‰‡ç»“æœ" çš„å·¥ä½œè¡¨ã€‚
#    - å­—æ®µ: "æŠ“å–æ—¶é—´", "æœç´¢URL", "ç¼©ç•¥å›¾URL", "æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„", "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥", "åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥", "ç‚¹èµæ•°", "çˆ±å¿ƒæ•°", "ç¬‘å“­æ•°", "ä¼¤å¿ƒæ•°", "æ‰“èµæ•°", **"å…³é”®è¯"**ã€‚
#    - è¶…é“¾æ¥å¤„ç†: ä¸ºæœç´¢URLã€åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥å’Œ**æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥**æ·»åŠ å¯ç‚¹å‡»çš„è¶…é“¾æ¥ï¼Œ**å¹¶ä¿®æ”¹æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥çš„æ˜¾ç¤ºæ–‡æœ¬**ã€‚
#    - åˆ—å®½è‡ªé€‚åº”: æ ¹æ®åˆ—å†…å®¹çš„æœ€å¤§é•¿åº¦è‡ªåŠ¨è°ƒæ•´åˆ—å®½ã€‚
#
# 7. è„šæœ¬å…¥å£ç‚¹ (Script Entry Point - `if __name__ == '__main__':`):
#    - è¿è¡Œä¸»å¼‚æ­¥å‡½æ•°ã€‚
#    - é”™è¯¯å¤„ç†: æ•è·ç”¨æˆ·ä¸­æ–­å’Œæ‰€æœ‰æœªå¤„ç†çš„å¼‚å¸¸ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚
#    - æ¸…ç†å’Œè‡ªåŠ¨æ‰“å¼€æ–‡ä»¶ (`finally` å—): å…³é—­æ‰€æœ‰æ—¥å¿—å¤„ç†å™¨ï¼Œå¹¶å°è¯•è‡ªåŠ¨æ‰“å¼€ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶å’ŒExcelç»“æœæ–‡ä»¶ã€‚
#    - å…¼å®¹å¤šæ“ä½œç³»ç»Ÿ: è‡ªåŠ¨åˆ¤æ–­æ“ä½œç³»ç»Ÿå¹¶ä½¿ç”¨ç›¸åº”çš„å‘½ä»¤æ‰“å¼€æ–‡ä»¶ã€‚
#
# --- æŠ€æœ¯æ ˆ ---
# - Python 3.x
# - Playwright (å¼‚æ­¥æµè§ˆå™¨è‡ªåŠ¨åŒ–åº“)
# - BeautifulSoup4 (HTMLè§£æåº“)
# - aiohttp (å¼‚æ­¥HTTPå®¢æˆ·ç«¯ï¼Œç”¨äºå›¾ç‰‡ä¸‹è½½)
# - openpyxl (Excelæ–‡ä»¶è¯»å†™åº“)
# - asyncio (Pythonå¼‚æ­¥ç¼–ç¨‹æ¡†æ¶)
# - logging (Pythonå†…ç½®æ—¥å¿—æ¨¡å—)
# - os, json, datetime, traceback, subprocess, hashlib, re (Pythonæ ‡å‡†åº“)
# - aiofiles (å¼‚æ­¥æ–‡ä»¶æ“ä½œï¼Œç”¨äºä¿å­˜å›¾ç‰‡)
#
# --- è¿è¡Œç¯å¢ƒè¦æ±‚ ---
# - Pythonç¯å¢ƒå·²å®‰è£…ã€‚
# - ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„Pythonåº“: `pip install playwright beautifulsoup4 openpyxl aiohttp aiofiles`
# - è¿è¡Œ `playwright install` å®‰è£…æµè§ˆå™¨é©±åŠ¨ã€‚
# - éœ€è¦ä¸€ä¸ª `keywordTarget.txt` æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«è¦æœç´¢çš„å…³é”®è¯ï¼Œä¸€è¡Œä¸€ä¸ªã€‚ (åœ¨æ­¤è„šæœ¬ä¸­ï¼Œæ­¤æ–‡ä»¶ä¸å†æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºURLå›ºå®š)
# - **éœ€è¦ä¸€ä¸ª `cookies.json` æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æœ‰æ•ˆçš„ JSON æ ¼å¼çš„ cookies æ•°æ® (å¦‚æœéœ€è¦)ã€‚**
# - å¯é€‰é…ç½®ä»£ç† (`PROXY` å˜é‡)ã€‚
#
import os
import json
import asyncio
import aiohttp
from datetime import datetime
import traceback
import logging
import subprocess
from openpyxl import Workbook
from openpyxl.styles import Font
from openpyxl.utils import get_column_letter
from openpyxl.worksheet.hyperlink import Hyperlink # Import Hyperlink for more control
import hashlib
import aiofiles
import base64
import re

# --- FIX FOR NameError: name 'playwright' is not defined ---
import playwright.async_api as playwright_api
from playwright.async_api import async_playwright, expect
# --- END FIX ---

from bs4 import BeautifulSoup


# --- Configuration ---
PROXY = "http://127.0.0.1:10808" # Your proxy address
LOG_DIR = "logs" # Directory for logs
IMAGE_DIR_BASE = "images_civitai" # Base directory for images - Changed for Civitai
RESULTS_DIR = "results_civitai" # Directory for Excel results - Changed for Civitai
KEYWORD_TARGET_FILE = "urlTarget.txt" # File to store target URLs for Civitai
DOWNLOAD_HISTORY_FILE = "download_history_civitai.json" # New: File to store download history for deduplication - Changed for Civitai

# Generate a timestamp for unique filenames - Moved to the top of the script for global access
timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
log_filename = os.path.join(LOG_DIR, f"civitai_scraper_log_{timestamp}.txt") # Changed log filename
excel_filename = os.path.join(RESULTS_DIR, f"civitai_image_results_{timestamp}.xlsx") # Excel filename with timestamp


# --- Logging Setup ---
logger = logging.getLogger('civitai_scraper') # Changed logger name
logger.setLevel(logging.INFO)

# Create necessary directories if they don't exist
if not os.path.exists(IMAGE_DIR_BASE):
    os.makedirs(IMAGE_DIR_BASE) # Use makedirs to create intermediate directories if they don't exist
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
if not os.path.exists(RESULTS_DIR):
    os.makedirs(RESULTS_DIR)

# Create file handler for logging to a file
file_handler = logging.FileHandler(log_filename, encoding='utf-8')
file_handler.setLevel(logging.INFO)

# Create console handler for logging to console
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# Define log message format
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# Add handlers to the logger
logger.addHandler(file_handler)
logger.addHandler(console_handler)
# --- End Logging Setup ---

# Global list to store data for Excel export
all_search_results_data = []
# Lock for thread-safe access to all_search_results_data when multiple async tasks are running
data_lock = asyncio.Lock()

# Global dictionary to store download history: {md5_hash_of_image_content: local_file_path}
download_history = {}


# --- Helper function to calculate MD5 hash of bytes ---
def calculate_md5(data_bytes):
    """Calculates the MD5 hash of a given byte string."""
    return hashlib.md5(data_bytes).hexdigest()

# --- Helper functions to load/save download history ---
def load_download_history(filepath):
    """Loads the download history from a JSON file."""
    global download_history
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                download_history = json.load(f)
            logger.info(f"Loaded download history from {filepath}")
        except json.JSONDecodeError:
            logger.warning(f"Warning: '{filepath}' contains invalid JSON. Starting with empty history.")
            download_history = {}
        except Exception as e:
            logger.error(f"Error loading download history from '{filepath}': {e}\n{traceback.format_exc()}")
            download_history = {}
    else:
        logger.info(f"Download history file '{filepath}' not found. Starting with empty history.")
        download_history = {}

def save_download_history(filepath):
    """Saves the current download history to a JSON file."""
    global download_history
    try:
        dir_name = os.path.dirname(filepath)
        if dir_name:  # åªæœ‰ç›®å½•éç©ºæ‰åˆ›å»º
            os.makedirs(dir_name, exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(download_history, f, indent=4)
        logger.info(f"Saved download history to {filepath}")
    except Exception as e:
        logger.error(f"Error saving download history to '{filepath}': {e}\n{traceback.format_exc()}")

# --- New helper function to read URLs from file ---
# Renamed from read_keywords_from_file as we are now reading URLs
def read_urls_from_file(filepath):
    """Reads URLs from a text file, one URL per line."""
    urls = []
    if not os.path.exists(filepath):
        logger.error(f"Error: Target URL file '{filepath}' not found. Please create it with one URL per line. For example, add 'https://civitai.com/images?tags=4' to '{filepath}'.")
        return []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and url.startswith("http"): # Basic validation for non-empty and http(s) URL
                    urls.append(url)
        if not urls:
            logger.warning(f"Warning: Target URL file '{filepath}' is empty or contains no valid URLs.")
        return urls
    except Exception as e:
        logger.error(f"Error reading URLs from '{filepath}': {e}\n{traceback.format_exc()}")
        return []
# --- End new helper function ---


async def process_image_data(image_url, base_folder_path):
    """
    Processes an image URL (external or data:image), handles deduplication using MD5,
    downloads if necessary, and returns the local file path and the content MD5 hash.
    If the image (by content MD5) is already in history, it returns the existing path.
    """
    local_filename = None
    image_content_md5 = None
    image_bytes = None

    if not image_url:
        logger.warning("Empty image URL skipped.")
        return None, None

    # Civitai images are typically external, so we'll focus on http/https
    if image_url.startswith('http'):
        url_without_query = image_url.split('?')[0]
        file_extension = url_without_query.split('.')[-1].lower()
        if not file_extension or len(file_extension) > 5 or not file_extension.isalpha():
            file_extension = 'jpg' # Default to jpg if no clear extension

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(image_url, proxy=PROXY if PROXY else None, timeout=30.0) as response:
                    response.raise_for_status()
                    image_bytes = await response.read()
                    image_content_md5 = calculate_md5(image_bytes)

                    # Deduplication check
                    if image_content_md5 in download_history:
                        existing_path = download_history[image_content_md5]
                        logger.info(f"Downloaded image content (MD5: {image_content_md5}) already exists at: {existing_path}. Skipping save and using existing path.")
                        return existing_path, image_content_md5

                    local_filename = os.path.join(base_folder_path, f"{image_content_md5}.{file_extension}")

                    os.makedirs(os.path.dirname(local_filename), exist_ok=True)

                    async with aiofiles.open(local_filename, 'wb') as f:
                        await f.write(image_bytes)
                    logger.info(f"Image downloaded and saved: {local_filename}")

                    download_history[image_content_md5] = local_filename
                    return local_filename, image_content_md5

        except aiohttp.ClientResponseError as e:
            logger.error(f"HTTP error {e.status} downloading image {image_url}: {e.message}")
        except aiohttp.ClientConnectorError as e:
            logger.error(f"Network error (ClientConnectorError) downloading image {image_url}: {e}")
        except asyncio.TimeoutError:
            logger.error(f"Timeout error downloading image {image_url}")
        except Exception as e:
            logger.error(f"Unexpected error downloading image {image_url}: {e}\n{traceback.format_exc()}")
    else:
        logger.warning(f"Unsupported image URL format (not http/https): {image_url[:100]}...")

    return None, None


async def performCivitaiImageScrape(context, target_url):
    async_name = asyncio.current_task().get_name()

    # --- Load and add cookies to browser context ---
    try:
        if os.path.exists("cookies.json"):
            with open("cookies.json", "r", encoding="utf-8") as f:
                cookies = json.load(f)
                for cookie in cookies:
                    cookie_same_site = {'strict': 'Strict', 'Lax': 'Lax', 'none': 'None'}.get(cookie.get('sameSite'), None)
                    if cookie_same_site in ['Strict', 'Lax', 'None']:
                        cookie['sameSite'] = cookie_same_site
                    else:
                        if 'sameSite' in cookie:
                            del cookie['sameSite']
                await context.add_cookies(cookies)
            logger.info(f"{async_name} -> Cookies loaded and added to context.")
        else:
            logger.warning(f"{async_name} -> Warning: cookies.json not found. Proceeding without cookies. Please ensure 'cookies.json' exists if needed.")
    except json.JSONDecodeError:
        logger.error(f"{async_name} -> Error: Invalid JSON in cookies.json. Please check the file format. Full traceback:\n{traceback.format_exc()}")
    except Exception as e:
        logger.error(f"{async_name} -> Unexpected error loading cookies: {e}\n{traceback.format_exc()}")
    # --- End cookie loading ---

    page = await context.new_page()

    try:
        logger.info(f"{async_name} -> Navigating to {target_url}")
        await page.goto(target_url, timeout=60000, wait_until="domcontentloaded")
        logger.info(f"{async_name} -> Successfully navigated to {target_url}")
    except playwright_api.TimeoutError:
        logger.error(f"{async_name} -> Error: Page.goto timed out for {target_url} after 60 seconds. Check network or proxy.")
        await page.close()
        return
    except Exception as e:
        logger.error(f"{async_name} -> An unexpected error occurred during navigation to {target_url}: {e}")
        await page.close()
        return

    # Create a general Civitai image folder, as keywords are not used for subfolders here
    civitai_image_folder_path = os.path.join(IMAGE_DIR_BASE, "downloaded_images")
    if not os.path.exists(civitai_image_folder_path):
        os.makedirs(civitai_image_folder_path)
        logger.info(f"{async_name} -> Created base image folder for Civitai: {civitai_image_folder_path}")

    scrape_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    processed_image_detail_urls = set() # Set to keep track of processed image detail page URLs to avoid redundant processing

    # --- Dynamic Scrolling and Scraping ---
    scroll_attempts = 0
    max_scroll_attempts = 30 # Increased max attempts for more thorough scraping
    last_known_image_count = 0
    no_new_images_count = 0 # Counter for how many times no new images were found after scroll

    # Selector for the main image grid container
    image_boxes_selector = '#main > div > div > div > main > div.z-10.m-0.flex-1.mantine-1avyp1d > div > div > div > div:nth-child(2) > div.mx-auto.flex.justify-center.gap-4'
    
    # Keyword input selector
    keyword_input_selector = '#mantine-r6rf' # User provided CSS selector

    # Extract keyword from the input field if it exists
    current_keyword = "N/A"
    try:
        keyword_input_element = page.locator(keyword_input_selector)
        if await keyword_input_element.is_visible():
            input_value = await keyword_input_element.get_attribute('value')
            if input_value:
                current_keyword = input_value
                logger.info(f"{async_name} -> Found keyword in input field: '{current_keyword}'")
    except Exception as e:
        logger.warning(f"{async_name} -> Could not find or extract keyword from input field '{keyword_input_selector}': {e}")


    while scroll_attempts < max_scroll_attempts:
        scroll_attempts += 1
        logger.info(f"{async_name} -> Scroll attempt {scroll_attempts}/{max_scroll_attempts}...")

        # Scroll to the bottom of the page
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        logger.info(f"{async_name} -> Scrolled to bottom. Waiting for content to load...")
        await asyncio.sleep(2) # Wait for content to load (increase if needed)

        # Optional: Scroll up slightly to ensure lazy-loaded images come into view
        # This simulates "ä¸Šä¸‹æ¥å›ä¸­ç­‰é€Ÿåº¦è¿›è¡Œç¿»åŠ¨"
        if scroll_attempts % 3 == 0:
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight * 0.7)')
            logger.info(f"{async_name} -> Scrolled up slightly (70%). Waiting 1 second...")
            await asyncio.sleep(1)

        # --- Data Extraction after each scroll ---
        try:
            # We need to get the outerHTML of each individual image box, not the container
            # The individual image box selector is:
            individual_image_box_selector = f'{image_boxes_selector} > div' # Each direct child div within the main grid
            
            # Wait for some elements to be attached, ensuring the DOM is somewhat ready
            await page.wait_for_selector(individual_image_box_selector, state='attached', timeout=15000)
            
            # Get all outer HTML of the current image boxes
            image_box_elements = await page.query_selector_all(individual_image_box_selector)
            
            current_image_count = len(image_box_elements)
            logger.info(f"{async_name} -> Found {current_image_count} image results on page after scroll attempt {scroll_attempts}.")

            newly_processed_this_scroll = 0

            for element in image_box_elements:
                html_content = await element.evaluate('node => node.outerHTML')
                soup = BeautifulSoup(html_content, "html.parser")

                result_data = {
                    "æŠ“å–æ—¶é—´": scrape_timestamp,
                    "æœç´¢URL": target_url,
                    "ç¼©ç•¥å›¾URL": "",
                    "æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„": "",
                    "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥": "",
                    "åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥": "",
                    "ç‚¹èµæ•°": 0,
                    "çˆ±å¿ƒæ•°": 0,
                    "ç¬‘å“­æ•°": 0,
                    "ä¼¤å¿ƒæ•°": 0,
                    "æ‰“èµæ•°": 0,
                    "å…³é”®è¯": current_keyword # Add the extracted keyword
                }

                # Extract original image detail page link first for deduplication
                thumbnail_img_element = soup.select_one('img')
                original_page_link_element = thumbnail_img_element.find_parent('a') if thumbnail_img_element else None
                original_page_url = original_page_link_element.get('href') if original_page_link_element else ""
                if original_page_url and not original_page_url.startswith('http'):
                    original_page_url = f"https://civitai.com{original_page_url}"
                result_data["åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥"] = original_page_url

                # Deduplication check for already processed image *detail pages*
                if original_page_url and original_page_url in processed_image_detail_urls:
                    # logger.debug(f"{async_name} -> Skipping already processed image detail URL: {original_page_url}")
                    continue # Skip this entry if its detail page URL was already processed
                
                # Add to processed set ONLY IF we are going to process it
                if original_page_url:
                    processed_image_detail_urls.add(original_page_url)
                    newly_processed_this_scroll += 1


                # Now extract other details
                thumbnail_url = thumbnail_img_element.get('src') if thumbnail_img_element else ""
                result_data["ç¼©ç•¥å›¾URL"] = thumbnail_url
                
                # Process thumbnail image (download or use from history)
                if thumbnail_url:
                    local_image_path, image_md5 = await process_image_data(thumbnail_url, civitai_image_folder_path)
                    if local_image_path:
                        result_data["æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„"] = os.path.abspath(local_image_path)
                        if os.name == 'nt':
                            formatted_local_path = result_data['æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„'].replace('\\', '/')
                            result_data["æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥"] = f"file:///{formatted_local_path}"
                        else:
                            result_data["æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥"] = f"file://{result_data['æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„']}"
                
                # Extract emoji data (5 buttons as per latest info)
                # Selector for the emoji container within the individual image box
                # It's 'div:nth-child(2) > div' relative to the 'current_image_box'
                emoji_container = soup.select_one('div:nth-child(2) > div')
                if emoji_container:
                    reaction_buttons = emoji_container.select('button')
                    for button in reaction_buttons:
                        count_span = button.find('span', class_=re.compile(r'mantine-h\w+'))
                        count = int(count_span.get_text(strip=True)) if count_span and count_span.get_text(strip=True).isdigit() else 0
                        
                        svg_element = button.find('svg')
                        
                        # --- Heuristics for SVG icons ---
                        # These are based on common Civitai icon patterns and might need refinement.
                        # The 'd' attribute of the <path> tag within the SVG is often unique for an icon.
                        
                        # Thumbs Up (ç‚¹èµ)
                        if svg_element and any(
                            'M13 14h-1.5a1 1 0 01-1-1V6.5a1 1 0 011-1H13a1 1 0 011 1V13a1 1 0 01-1 1z' in str(p) or # One example path
                            'M7 11.25V9.5h-1.5' in str(p) # Another common thumbs up path
                            for p in svg_element.find_all('path')
                        ):
                            result_data["ç‚¹èµæ•°"] = count
                        # Heart (çˆ±å¿ƒ)
                        elif svg_element and any(
                            'M8 1.314C12.438-3.248 23.534 4.735 8 15-7.534 4.735 3.562-3.248 8 1.314z' in str(p) or # Standard heart path
                            'M16 8s-3.5 1.5-6.5 1.5S3 8 3 8l-.5-.5a4.7 4.7 0 010-6.7' in str(p) # Another possible heart path segment
                            for p in svg_element.find_all('path')
                        ):
                            result_data["çˆ±å¿ƒæ•°"] = count
                        # Crying-Laughing (ç¬‘å“­) - Often a specific face
                        elif svg_element and any(
                            'M8 15A7 7 0 108 1a7 7 0 000 14zm0 1A8 8 0 108 0a8 8 0 000 16z' in str(p) and # Face outline
                            'M7 8.5C7 8.224 7.224 8 7.5 8h1C8.776 8 9 8.224 9 8.5v.5H7V8.5z' in str(p) # Eye (part of crying face)
                            for p in svg_element.find_all('path')
                        ) or ('ğŸ˜‚' in button.get_text()): # Fallback for unicode if present
                            result_data["ç¬‘å“­æ•°"] = count
                        # Sad (ä¼¤å¿ƒ) - Specific face
                        elif svg_element and any(
                            'M8 15A7 7 0 108 1a7 7 0 000 14zm0 1A8 8 0 108 0a8 8 0 000 16z' in str(p) and # Face outline
                            'M5 10.5h6a.5.5 0 010 1H5a.5.5 0 010-1z' in str(p) # Sad mouth line
                            for p in svg_element.find_all('path')
                        ) or ('ğŸ˜¢' in button.get_text()): # Fallback for unicode if present
                            result_data["ä¼¤å¿ƒæ•°"] = count
                        # Yellow Lightning Bolt / Tip (æ‰“èµ)
                        elif svg_element and any(
                            'M11.5 1h-8A1.5 1.5 0 002 2.5v11A1.5 1.5 0 003.5 15h8A1.5 1.5 0 0013 13.5v-11A1.5 1.5 0 0011.5 1zM12 2.5a.5.5 0 00-.5-.5h-8a.5.5 0 00-.5.5v11a.5.5 0 00.5.5h8a.5.5 0 00.5-.5v-11zM11 6a.5.5 0 00-.5.5v4a.5.5 0 001 0v-4A.5.5 0 0011 6zM5.5 6a.5.5 0 00-.5.5v4a.5.5 0 001 0v-4A.5.5 0 005.5 6z' in str(p) or # Common lightning bolt shape
                            'M6.5 1.5l-4 8h5l-1 5 7-9h-5l2-4z' in str(p) # Another lightning bolt shape
                            for p in svg_element.find_all('path')
                        ):
                            result_data["æ‰“èµæ•°"] = count
                        else:
                            logger.debug(f"{async_name} -> Unknown reaction button found. Text: '{button.get_text(strip=True)}'. SVG excerpt: {str(svg_element)[:200] if svg_element else 'N/A'}")
                else:
                    logger.warning(f"{async_name} -> No emoji container (div:nth-child(2)>div) found for an image result.")

                # Only log and add if it's a newly processed item
                if original_page_url and original_page_url in processed_image_detail_urls: # Double check after adding
                    async with data_lock:
                        all_search_results_data.append(result_data)

            if newly_processed_this_scroll == 0 and current_image_count > 0:
                no_new_images_count += 1
                logger.info(f"{async_name} -> No new images processed this scroll. Consecutive no new images: {no_new_images_count}")
                if no_new_images_count >= 3: # If no new images for 3 consecutive scrolls, assume end
                    logger.info(f"{async_name} -> Reached end of content. Stopping scrolling.")
                    break
            else:
                no_new_images_count = 0 # Reset counter if new images were found

        except playwright_api.TimeoutError:
            logger.error(f"{async_name} -> Error: Image result elements not found on page after scroll within timeout. This might indicate no more content or a selector issue.")
            break # Stop scrolling if elements cannot be found
        except Exception as e:
            logger.error(f"{async_name} -> An unexpected error occurred during image results processing after scroll: {e}\n{traceback.format_exc()}")
            break # Stop scrolling on unexpected errors

    await page.close()
    logger.info(f"{async_name} -> Page closed for {target_url}.")


async def main():
    load_download_history(DOWNLOAD_HISTORY_FILE)

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            proxy={"server": PROXY} if PROXY else None,
            timeout=60000
        )
        context = await browser.new_context(
            viewport={'width': 2560, 'height': 1440}
        )

        target_urls = read_urls_from_file(KEYWORD_TARGET_FILE)
        if not target_urls:
            logger.error(f"No valid URLs found in {KEYWORD_TARGET_FILE}. Please add URLs to scrape.")
            await browser.close()
            return

        tasks = [performCivitaiImageScrape(context, url) for url in target_urls]

        await asyncio.gather(*tasks)

        await browser.close()
        logger.info("Browser closed. Script finished scraping data.")

    # --- Excel Export Logic ---
    wb = Workbook()
    ws = wb.active
    ws.title = "Civitaiå›¾ç‰‡ç»“æœ"

    # Define headers for the Civitai results sheet - Added "å…³é”®è¯"
    headers = ["æŠ“å–æ—¶é—´", "æœç´¢URL", "ç¼©ç•¥å›¾URL", "æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„", "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥", "åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥", "ç‚¹èµæ•°", "çˆ±å¿ƒæ•°", "ç¬‘å“­æ•°", "ä¼¤å¿ƒæ•°", "æ‰“èµæ•°", "å…³é”®è¯"]
    ws.append(headers)

    hyperlink_font = Font(color="0000FF", underline="single")

    for row_data in all_search_results_data:
        row = []
        for header in headers:
            # For "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥", we will set the cell value and hyperlink separately
            if header == "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥":
                row.append("ç‚¹å‡»æ‰“å¼€ç¼©ç•¥å›¾") # This will be the display text initially
            else:
                row.append(row_data.get(header, ""))
        ws.append(row)

        current_row_idx = ws.max_row

        # Apply hyperlink for "æœç´¢URL"
        search_url = row_data.get("æœç´¢URL")
        if search_url:
            cell_search_url = ws.cell(row=current_row_idx, column=headers.index("æœç´¢URL") + 1)
            cell_search_url.value = search_url
            cell_search_url.hyperlink = search_url
            cell_search_url.font = hyperlink_font

        # Apply hyperlink for "ç¼©ç•¥å›¾URL"
        thumbnail_url = row_data.get("ç¼©ç•¥å›¾URL")
        if thumbnail_url:
            cell_thumbnail_url = ws.cell(row=current_row_idx, column=headers.index("ç¼©ç•¥å›¾URL") + 1)
            cell_thumbnail_url.value = thumbnail_url
            cell_thumbnail_url.hyperlink = thumbnail_url
            cell_thumbnail_url.font = hyperlink_font
            
        # Apply hyperlink for "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥" with new display text and actual file link
        local_image_hyperlink_url = row_data.get("æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥") # This is the "file:///" URL
        if local_image_hyperlink_url:
            cell_local_image_hyperlink = ws.cell(row=current_row_idx, column=headers.index("æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æ¥") + 1)
            cell_local_image_hyperlink.value = "ç‚¹å‡»æ‰“å¼€ç¼©ç•¥å›¾" # Set display text
            # Create a Hyperlink object, linking to the file:// URL
            cell_local_image_hyperlink.hyperlink = Hyperlink(ref=local_image_hyperlink_url)
            cell_local_image_hyperlink.font = hyperlink_font


        # Apply hyperlink for "åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥"
        original_page_link = row_data.get("åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥")
        if original_page_link:
            cell_original_page_link = ws.cell(row=current_row_idx, column=headers.index("åŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æ¥") + 1)
            cell_original_page_link.value = original_page_link
            cell_original_page_link.hyperlink = original_page_link
            cell_original_page_link.font = hyperlink_font

    for col_idx, header in enumerate(headers):
        max_length = len(header)
        column_letter = get_column_letter(col_idx + 1)
        # Iterate over all cells in the column to find the true max_length, considering the new hyperlink text
        for r_idx in range(1, ws.max_row + 1):
            cell_value = ws.cell(row=r_idx, column=col_idx + 1).value
            if cell_value:
                cell_len = len(str(cell_value))
                if cell_len > max_length:
                    max_length = cell_len

        adjusted_width = (max_length + 2) * 1.2
        if adjusted_width > 100: # Cap column width
            adjusted_width = 100
        ws.column_dimensions[column_letter].width = adjusted_width

    wb.save(excel_filename)
    logger.info(f"Results saved to Excel: {excel_filename}")
    # --- END Excel Export Logic ---

    save_download_history(DOWNLOAD_HISTORY_FILE)


# --- æ»šåŠ¨ç›¸å…³å‡½æ•°å»ºè®®æ”¾åœ¨è¿™é‡Œ ---
async def civitai_basic_scroll(url="https://civitai.com/images?tags=4", scroll_times=30, sleep_sec=2):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={'width': 2560, 'height': 1440}
        )
        page = await context.new_page()
        await page.goto(url)
        for _ in range(scroll_times):
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(sleep_sec)
        await browser.close()

async def civitai_scroll_all_elements(url="https://civitai.com/images?tags=4", max_rounds=50, sleep_sec=1):
    js_scroll_all = """
    document.querySelectorAll('*').forEach(function(el) {
      if (el.scrollHeight > el.clientHeight) el.scrollTop += 40;
    });
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()
        await page.goto(url)
        last_height = 0
        for _ in range(max_rounds):
            await page.evaluate(js_scroll_all)
            await asyncio.sleep(sleep_sec)
            curr_height = await page.evaluate("document.body.scrollHeight")
            if curr_height == last_height:
                print("é¡µé¢é«˜åº¦æœªå¢åŠ ï¼Œå¯èƒ½å·²æ»šåˆ°åº•éƒ¨ï¼Œåœæ­¢æ»šåŠ¨ã€‚")
                break
            last_height = curr_height
        await browser.close()
# --- END æ»šåŠ¨ç›¸å…³å‡½æ•° ---


# --- New function to scroll and activate all elements on the page ---
async def civitai_scroll_all_elements(url="https://civitai.com/images?tags=4", max_rounds=50, sleep_sec=1):
    js_scroll_all = """
    document.querySelectorAll('*').forEach(function(el) {
      if (el.scrollHeight > el.clientHeight) el.scrollTop += 40;
    });
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()
        await page.goto(url)
        last_height = 0
        for _ in range(max_rounds):
            await page.evaluate(js_scroll_all)
            await asyncio.sleep(sleep_sec)
            # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦è¿˜åœ¨å¢åŠ 
            curr_height = await page.evaluate("document.body.scrollHeight")
            if curr_height == last_height:
                print("é¡µé¢é«˜åº¦æœªå¢åŠ ï¼Œå¯èƒ½å·²æ»šåˆ°åº•éƒ¨ï¼Œåœæ­¢æ»šåŠ¨ã€‚")
                break
            last_height = curr_height
        await browser.close()

# ç”¨æ³•ï¼šasyncio.run(civitai_scroll_all_elements())
# --- END new function ---


async def inject_and_click_scroll_btn(url="https://civitai.com/images?tags=4", max_retry=10, retry_interval=1):
    with open("æ§åˆ¶å°æ³¨å…¥ç‰ˆ.js", "r", encoding="utf-8") as f:
        content_js_code = f.read()
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()
        page.on("console", lambda msg: print("PAGE LOG:", msg.text))
        await page.goto(url)
        await page.wait_for_load_state("networkidle")

        # 1. å…ˆç”¨ evaluate æ³¨å…¥
        for i in range(max_retry):
            print(f"[evaluate] æ³¨å…¥å°è¯• {i+1}/{max_retry} ...")
            await page.evaluate(content_js_code)
            #await page.evaluate("createMenu()")
            try:
                await page.wait_for_selector("#ultra-scroll-menu-btn", timeout=1000)
                print("[evaluate] æŒ‰é’®å·²æˆåŠŸæ³¨å…¥ï¼")
                break
            except Exception:
                print("[evaluate] æŒ‰é’®æœªå‡ºç°ï¼Œç»§ç»­å°è¯•æ³¨å…¥...")
                await asyncio.sleep(retry_interval)
        else:
            print("[evaluate] å¤šæ¬¡æ³¨å…¥åä»æœªæ£€æµ‹åˆ°æŒ‰é’®ï¼Œå°è¯• add_init_script...")

            # 2. ç”¨ add_init_script æ³¨å…¥
            await page.add_init_script(content_js_code)
            await page.reload()
            try:
                await page.wait_for_selector("#ultra-scroll-menu-btn", timeout=5000)
                print("[add_init_script] æŒ‰é’®å·²æˆåŠŸæ³¨å…¥ï¼")
            except Exception:
                print("[add_init_script] æŒ‰é’®æœªå‡ºç°ï¼Œå°è¯• expose_function...")

                # 3. ç”¨ expose_function æ–¹å¼ï¼ˆä¸æ¨èï¼Œä½†å¯æ¼”ç¤ºï¼‰
                # è¿™é‡Œåªèƒ½æ¼”ç¤º expose_function çš„ç”¨æ³•ï¼Œå®é™…æ’å…¥æŒ‰é’®è¿˜æ˜¯å¾—ç”¨ evaluate
                # expose_function é€‚åˆé¡µé¢ä¸»åŠ¨è°ƒç”¨ Pythonï¼Œä¸é€‚åˆç›´æ¥æ’å…¥æŒ‰é’®

                print("[expose_function] æš‚æ— é€‚ç”¨åœºæ™¯ï¼Œå»ºè®®ç”¨ evaluate æˆ– add_init_scriptã€‚")
                await browser.close()
                return

        # æŒ‰é’®å‡ºç°åè‡ªåŠ¨ç‚¹å‡»
        await page.click("#ultra-scroll-menu-btn")
        await asyncio.sleep(10)
        await page.click("#ultra-scroll-menu-btn")
        await asyncio.sleep(2)
        await browser.close()

# ç”¨æ³•
# asyncio.run(inject_and_click_scroll_btn())


if __name__ == '__main__':
    try:
        # åªæµ‹è¯•æŒ‰é’®æ³¨å…¥
        asyncio.run(inject_and_click_scroll_btn())
    except KeyboardInterrupt:
        logger.info("Script interrupted by user.")
    except Exception as e:
        logger.critical(f"An unhandled error occurred in main: {e}\n{traceback.format_exc()}")
    finally:
        for handler in logger.handlers[:]:
            try:
                handler.flush()
                handler.close()
                logger.removeHandler(handler)
            except Exception as e:
                print(f"Error closing log handler: {e}")

        # Automatically open the log file for review
        try:
            if os.path.exists(log_filename):
                print(f"Attempting to open log file: {log_filename}")
                if os.name == 'nt':
                    os.startfile(log_filename)
                elif os.uname().sysname == 'Darwin':
                    subprocess.run(['open', log_filename])
                else:
                    subprocess.run(['xdg-open', log_filename])
            else:
                print(f"Log file not found: {log_filename}")
        except Exception as e:
            print(f"Error opening log file {log_filename}: {e}")

        # Automatically open the Excel file for review
        try:
            if os.path.exists(excel_filename):
                print(f"Attempting to open Excel file: {excel_filename}")
                if os.name == 'nt':
                    os.startfile(excel_filename)
                elif os.uname().sysname == 'Darwin':
                    subprocess.run(['open', excel_filename])
                else:
                    subprocess.run(['xdg-open', excel_filename])
            else:
                print(f"Excel file not found: {excel_filename}")
        except Exception as e:
            print(f"Error opening Excel file {excel_filename}: {e}")